# ğŸ”„ Model Loading & Architecture Diagrams

## 1. Bot Startup Flow

```
START
  â†“
main.py execution
  â†“
load .env variables
  â”œâ”€ DISCORD_TOKEN
  â”œâ”€ LLM_MODEL_NAME â†’ "Qwen/Qwen3-1.7B-Instruct"
  â”œâ”€ LLM_MODEL_PATH â†’ "./models/Qwen3-1.7B-Instruct"
  â”œâ”€ LLM_DEVICE â†’ "cpu"
  â””â”€ LLM_DTYPE â†’ "float32"
  â†“
bot.run(DISCORD_TOKEN)
  â†“
on_ready() event triggered
  â”œâ”€ print("Loading LLM model...")
  â”œâ”€ load_llm() called
  â”‚   â”œâ”€ Check if LLM_MODEL_PATH exists locally?
  â”‚   â”‚   â”œâ”€ YES â†’ Load from ./models/ (fast, ~2-5s)
  â”‚   â”‚   â””â”€ NO â†’ Download from HuggingFace (slow, ~10-20 min)
  â”‚   â”œâ”€ Load tokenizer
  â”‚   â”œâ”€ Load model onto device (cpu/cuda)
  â”‚   â””â”€ Return True/False
  â”œâ”€ Load Discord cogs
  â””â”€ Sync slash commands
  â†“
âœ… Bot READY - Waiting for commands
```

## 2. Game Turn Flow

```
Player types /newgame [scenario]
  â†“
game_commands.py â†’ create_game()
  â”œâ”€ Generate random map (map_generator.py)
  â”œâ”€ Initialize player stats
  â”œâ”€ Create Discord channel for game
  â””â”€ Display game dashboard (game_ui.py)
  â†“
Show embed with:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¨ HORROR HOTEL        â”‚
â”‚  Floor: 3, Room: 5      â”‚
â”‚                         â”‚
â”‚  ğŸ•·ï¸ HP: [â–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 70    â”‚
â”‚  ğŸ˜¨ Sanity: [â–ˆâ–ˆâ–ˆâ–ˆâ–‘] 80  â”‚
â”‚                         â”‚
â”‚  [âš”ï¸ Attack] [ğŸƒ Flee]  â”‚
â”‚  [ğŸ” Search]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
Players select action
  â”œâ”€ Click button
  â”œâ”€ Button callback â†’ game_ui.py
  â”œâ”€ Send ephemeral message "Action received"
  â”œâ”€ Register action in game_engine.py
  â””â”€ Check if all players acted or timeout?
  â†“
If all players acted OR timeout reached:
  â”œâ”€ Gather all player actions
  â”œâ”€ Resolve actions (calculate damage, etc)
  â”œâ”€ Call describe_scene() â†’ llm_service.py
  â”‚   â”œâ”€ LLM generates scene description
  â”‚   â”œâ”€ Runs in executor (non-blocking)
  â”‚   â””â”€ Returns description text
  â”œâ”€ Update player stats
  â”œâ”€ Generate new embed
  â””â”€ Loop back to "Show embed"
  â†“
Player quits or game ends
  â””â”€ END GAME
```

## 3. LLM Service Architecture

```
describe_scene(context_keywords)
  â†“
  [Check Model Loaded]
  â”œâ”€ get_llm() â†’ Returns global llm_model
  â”œâ”€ get_tokenizer() â†’ Returns global llm_tokenizer
  â””â”€ If None â†’ Return fallback text "The air is thick..."
  â†“
  [Prepare Input]
  â”œâ”€ prompt = f"Describe: {context_keywords}"
  â”œâ”€ messages = [{"role": "user", "content": prompt}]
  â”œâ”€ text = tokenizer.apply_chat_template(messages)
  â””â”€ model_inputs = tokenizer(text, return_tensors="pt")
  â†“
  [Run in Executor] (non-blocking async)
  â””â”€ generate():
      â”œâ”€ model.generate(max_new_tokens=150, ...)
      â”œâ”€ torch.no_grad() to save memory
      â”œâ”€ Return decoded text
      â””â”€ ~20-45 seconds on CPU
  â†“
  [Post-Process]
  â”œâ”€ Strip special tokens
  â”œâ”€ Clean whitespace
  â””â”€ Return description
  â†“
  description â†’ game_engine â†’ embed â†’ Discord
```

## 4. Model Loading Path Comparison

### Scenario 1: First Time Run (No Local Model)

```
start
  â†“
check ./models/Qwen3-1.7B-Instruct/
  â†“
Not found!
  â†“
Download from HuggingFace:
â”œâ”€ Qwen/Qwen3-1.7B-Instruct (tokenizer)
â””â”€ Qwen/Qwen3-1.7B-Instruct (model)
  â†“
Save to ./models/Qwen3-1.7B-Instruct/
  â”œâ”€ config.json
  â”œâ”€ generation_config.json
  â”œâ”€ model.safetensors (or .bin)
  â”œâ”€ tokenizer.json
  â”œâ”€ tokenizer.model
  â”œâ”€ tokenizer_config.json
  â””â”€ other files
  â†“
Model loaded, ready to use
(Next time: Use cached version)
```

### Scenario 2: Cached Model Run

```
start
  â†“
check ./models/Qwen3-1.7B-Instruct/
  â†“
Found! Load immediately
  â”œâ”€ Load tokenizer from disk (~500ms)
  â”œâ”€ Load model weights (~3-5s on CPU)
  â””â”€ Ready to generate
  â†“
Model loaded, ready to use
(Instant, no download needed)
```

## 5. File Organization

```
horror_bot_project/
â”‚
â”œâ”€â”€ .env  â† Model configuration (DYNAMIC!)
â”‚   â””â”€ LLM_MODEL_NAME="Qwen/Qwen3-1.7B-Instruct"
â”‚   â””â”€ LLM_MODEL_PATH="./models/Qwen3-1.7B-Instruct"
â”‚   â””â”€ LLM_DEVICE="cpu"
â”‚   â””â”€ LLM_DTYPE="float32"
â”‚
â”œâ”€â”€ horror_bot/
â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â””â”€ Call load_llm() on startup
â”‚   â”‚
â”‚   â”œâ”€â”€ services/llm_service.py  â† Core LLM wrapper
â”‚   â”‚   â”œâ”€ load_llm()
â”‚   â”‚   â”œâ”€ get_llm()
â”‚   â”‚   â”œâ”€ get_tokenizer()
â”‚   â”‚   â””â”€ describe_scene()
â”‚   â”‚
â”‚   â”œâ”€â”€ models/  â† Model storage (auto-created)
â”‚   â”‚   â””â”€â”€ Qwen3-1.7B-Instruct/
â”‚   â”‚       â”œâ”€â”€ config.json
â”‚   â”‚       â”œâ”€â”€ model.safetensors (~3.2GB)
â”‚   â”‚       â”œâ”€â”€ tokenizer.json
â”‚   â”‚       â””â”€â”€ ... (other files)
â”‚   â”‚
â”‚   â”œâ”€â”€ cogs/
â”‚   â”œâ”€â”€ database/
â”‚   â””â”€â”€ data/
â”‚
â”œâ”€â”€ download_model.py  â† Initial model download script
â”‚
â””â”€â”€ setup_and_run.sh/bat  â† Auto-setup script
```

## 6. Memory & Performance Profile

### CPU Execution (Qwen3-1.7B float32)

```
Model Size: 3.2GB
Tokenizer: ~50MB
Context: ~512 tokens

RAM Usage:
â”œâ”€ Model weights: 3.2GB
â”œâ”€ Forward pass buffer: ~500MB
â”œâ”€ Attention cache: ~200MB
â””â”€ Total: ~3.9GB

Inference Time:
â”œâ”€ Tokenization: 10-20ms
â”œâ”€ Generation: 15-45s (150 tokens)
â”œâ”€ Decoding: 50-100ms
â””â”€ Total: 15-46s per description

Bottleneck: Model.generate() on CPU
```

### GPU Execution (Qwen3-1.7B float16)

```
Model Size: 1.6GB (half precision)
VRAM Usage: 3.5GB

Inference Time:
â”œâ”€ Tokenization: 5-10ms
â”œâ”€ Generation: 1-3s (150 tokens)
â”œâ”€ Decoding: 20-50ms
â””â”€ Total: 2-4s per description (10x faster!)

Bottleneck: None (GPU fast enough)
```

## 7. Error Handling Flow

```
describe_scene() called
  â†“
try:
  â”œâ”€ Get model and tokenizer
  â”‚   â””â”€ If None â†’ return fallback text
  â”œâ”€ Prepare inputs
  â”œâ”€ Run generate()
  â””â”€ Decode output
catch Exception:
  â”œâ”€ print error log
  â”œâ”€ return "A mysterious presence fills the room..."
  â””â”€ Continue game (graceful degradation)
  â†“
Game continues with AI-less descriptions
(Low quality but playable)
```

## 8. Model Switching Workflow

```
Want to switch model?
  â†“
1. Edit .env:
   LLM_MODEL_NAME="mistralai/Mistral-7B-Instruct-v0.1"
   LLM_MODEL_PATH="./models/Mistral-7B-Instruct"
  â†“
2. Run download_model.py
   (downloads new model to new path)
  â†“
3. Restart bot
   (load_llm() detects new path, loads new model)
  â†“
âœ… Bot now uses new model
(No code changes needed!)
```

## 9. Async Architecture

```
Discord Event (player clicks button)
  â†“
Button callback (async)
  â”œâ”€ Register action
  â”œâ”€ Send ephemeral response immediately
  â””â”€ Check if all acted
  â†“
If ready, start turn resolution
  â”œâ”€ Resolve game logic (fast)
  â””â”€ Call describe_scene()
  â†“
describe_scene() (async)
  â”œâ”€ Prepare input
  â””â”€ await loop.run_in_executor(None, generate)
      â”œâ”€ Blocks in thread pool (doesn't block bot)
      â”œâ”€ Bot can handle other events
      â””â”€ Returns description after 20-45s
  â†“
Update embed and send
  â†“
Bot continues handling other users
```

## 10. Configuration Precedence

```
System Default Values (hardcoded in code)
  â†‘
Environment Variables from .env (override defaults)
  â†‘
CLI Arguments (override .env if implemented)
  â†‘
Final Configuration Used
```

Example:
```python
LLM_MODEL_NAME = os.getenv("LLM_MODEL_NAME", "Qwen/Qwen3-1.7B-Instruct")
                                 â†‘                    â†‘
                           reads from .env      fallback default
```

---

## Summary

âœ… **Dynamic Loading** - Model path & name from .env  
âœ… **Auto-Download** - First-time setup automatic  
âœ… **Local Cache** - Subsequent runs use cached model  
âœ… **Async Execution** - Non-blocking bot operation  
âœ… **Graceful Fallback** - Works even if LLM disabled  
âœ… **Easy Switching** - Change model by editing .env  
