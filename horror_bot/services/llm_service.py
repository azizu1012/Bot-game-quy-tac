"""
HORROR BOT - LLM SERVICE (Per-user Dungeon Master)
Unified service for all LLM inference - turn narratives, per-player actions, encounters
"""

import asyncio
import json
import os
from dotenv import load_dotenv

try:
    from llama_cpp import Llama
except ImportError:
    print("‚ùå L·ªói: Ch∆∞a c√†i llama-cpp-python. H√£y ch·∫°y pip install -r requirements.txt")
    Llama = None

load_dotenv()

LLM_MODEL_PATH = os.getenv("LLM_MODEL_PATH")
n_threads = int(os.getenv("LLM_N_THREADS", "4"))
n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "8192"))

# Global model instance
_llm = None

def load_llm():
    """Load Qwen model once for entire bot lifecycle."""
    global _llm
    if _llm is not None:
        return True

    if not LLM_MODEL_PATH or not os.path.exists(LLM_MODEL_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y model t·∫°i: {LLM_MODEL_PATH}")
        print("üëâ H√£y ch·∫°y python download_model.py tr∆∞·ªõc.")
        return False

    try:
        print(f"üîÑ ƒêang load model GGUF (Threads: {n_threads}, Context: {n_ctx})...")
        _llm = Llama(
            model_path=LLM_MODEL_PATH,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_gpu_layers=0,  # Ch·∫°y thu·∫ßn CPU
            verbose=False
        )
        print("‚úÖ LLM Load th√†nh c√¥ng!")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói load model: {e}")
        return False


# ============================================================================
# PER-PLAYER ACTION PROCESSING (Free-Form Text Actions)
# ============================================================================

async def process_player_action(
    action_text: str,
    system_prompt: str,
    conversation_history: list = None
) -> str:
    """
    Process free-form player action through LLM.
    Per-user isolated context prevents cross-player state leakage.
    
    Args:
        action_text: What the player typed (e.g., "T√¥i t√¨m ki·∫øm quanh ph√≤ng")
        system_prompt: Per-player system prompt (location desc, stats, inventory)
        conversation_history: Last 10 messages (rolling window)
    
    Returns:
        JSON string with action outcome:
        {
            "success": bool,
            "description": str,
            "hp_change": int,
            "sanity_change": int,
            "new_location_id": str,
            "discovered_items": [str]
        }
    """
    if _llm is None:
        return json.dumps({
            "success": False,
            "description": "H·ªá th·ªëng AI ch∆∞a s·∫µn s√†ng.",
            "hp_change": 0,
            "sanity_change": 0,
            "new_location_id": "same",
            "discovered_items": []
        })

    if conversation_history is None:
        conversation_history = []

    # Build prompt with conversation history (last 5 messages for context)
    messages_text = ""
    for msg in conversation_history[-5:]:
        role = msg.get('role', 'user').upper()
        content = msg.get('content', '')
        messages_text += f"{role}: {content}\n"

    prompt = f"""<|im_start|>system
{system_prompt}<|im_end|>
{messages_text}
<|im_start|>user
{action_text}<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()

    def run_inference():
        try:
            output = _llm(
                prompt,
                max_tokens=500,
                stop=["<|im_end|>"],
                echo=False,
                temperature=0.8
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            print(f"‚ùå LLM inference error: {e}")
            return json.dumps({
                "success": False,
                "description": f"L·ªói: {e}",
                "hp_change": 0,
                "sanity_change": 0,
                "new_location_id": "same",
                "discovered_items": []
            })

    return await loop.run_in_executor(None, run_inference)


async def generate_encounter(
    action_description: str,
    player_name: str,
    other_players: list,
    scenario_type: str
) -> str:
    """
    Generate encounter scenario when 2+ players meet.
    Per-player isolated to avoid shared context issues.
    
    Args:
        action_description: What the first player did
        player_name: Name of first player
        other_players: List of other player names at location
        scenario_type: Scenario type for context
    
    Returns:
        Encounter description text (2-3 sentences)
    """
    if _llm is None:
        other_names = ", ".join(other_players)
        return f"B·∫°n g·∫∑p {other_names}. C·∫£m gi√°c r·∫•t k·ª≥ l·∫°..."

    prompt = f"""<|im_start|>system
B·∫°n l√† Dungeon Master kinh d√≠. M·ªôt t√¨nh hu·ªëng g·∫∑p g·ª° v·ª´a x·∫£y ra trong scenario {scenario_type}.

{player_name} v·ª´a {action_description}

C√°c nh√¢n v·∫≠t kh√°c t·∫°i ƒë√¢y: {', '.join(other_players)}

H√£y m√¥ t·∫£ c·∫£nh g·∫∑p g·ª° b·∫•t ng·ªù n√†y m·ªôt c√°ch kinh d√≠ v√† s·ªëng ƒë·ªông (2-3 c√¢u). 
Tone: b√≠ ·∫©n, cƒÉng th·∫≥ng, kh√¥ng ch·∫Øc ch·∫Øn.
<|im_end|>
<|im_start|>user
M√¥ t·∫£ c·∫£nh g·∫∑p g·ª° n√†y<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()

    def run_inference():
        try:
            output = _llm(
                prompt,
                max_tokens=200,
                stop=["<|im_end|>"],
                echo=False,
                temperature=0.9
            )
            return output['choices'][0]['text'].strip()
        except Exception as e:
            return f"B·∫°n g·∫∑p {', '.join(other_players)} trong t·ªëi t·ªëi..."

    return await loop.run_in_executor(None, run_inference)


# ============================================================================
# NARRATIVE GENERATION (Shared lore, rules, world-building)
# ============================================================================

async def describe_scene(keywords: list) -> str:
    """Generate scene description for narrative (optional, for global log)."""
    if _llm is None:
        return "Kh√¥ng gian tƒ©nh m·ªãch... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ ng·∫Øn (d∆∞·ªõi 50 t·ª´) d·ª±a tr√™n c√°c t·ª´ kh√≥a: {', '.join(keywords)}. Gi·ªçng vƒÉn u √°m, ƒë√°ng s·ª£.<|im_end|>
<|im_start|>user
M√¥ t·∫£ c·∫£nh n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>", "\n\n"],
            echo=False,
            temperature=0.7
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)


async def describe_scene_stream(keywords: list, callback=None) -> str:
    """Generate scene description v·ªõi streaming callback (g·ªçi callback t·ª´ng ph·∫ßn)."""
    if _llm is None:
        return "Kh√¥ng gian tƒ©nh m·ªãch... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ ng·∫Øn (d∆∞·ªõi 50 t·ª´) d·ª±a tr√™n c√°c t·ª´ kh√≥a: {', '.join(keywords)}. Gi·ªçng vƒÉn u √°m, ƒë√°ng s·ª£.<|im_end|>
<|im_start|>user
M√¥ t·∫£ c·∫£nh n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>", "\n\n"],
            echo=False,
            temperature=0.7
        )
        result = output['choices'][0]['text'].strip()
        
        if callback:
            sentences = result.split('. ')
            for i, sentence in enumerate(sentences):
                callback(sentence + ('.' if i < len(sentences) - 1 else ''))
        
        return result

    return await loop.run_in_executor(None, run_inference)


async def generate_dark_rules(scenario_type: str) -> str:
    """Generate a set of dark rules for the game scenario like Chinese novels."""
    if _llm is None:
        return "Kh√¥ng c√≥ quy t·∫Øc... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† t√°c gi·∫£ ti·ªÉu thuy·∫øt kinh d·ªã ch√¢u √Å. H√£y t·∫°o 3-4 quy t·∫Øc ma qu√°i, u √°m cho m·ªôt tr√≤ ch∆°i kinh d√≠ trong scenario '{scenario_type}'. Vi·∫øt d∆∞·ªõi d·∫°ng danh s√°ch v·ªõi tone mu·ªën r·ª£n ng∆∞·ªùi, huy·ªÅn b√≠, gi·ªëng nh∆∞ c√°c ti·ªÉu thuy·∫øt Trung Qu·ªëc c·ªï. Gi·ªØ ng·∫Øn g·ªçn, m·ªói quy t·∫Øc 1-2 c√¢u.<|im_end|>
<|im_start|>user
T·∫°o nh·ªØng quy t·∫Øc qu·ª∑ d·ªã cho scenario n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=200,
            stop=["<|im_end|>", "\n\n\n"],
            echo=False,
            temperature=0.8
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)


async def generate_waiting_room_message(num_players: int, total_slots: int = 8) -> str:
    """Generate a natural greeting for waiting room."""
    if _llm is None:
        return f"ƒêang ch·ªù ƒë·ªß ng∆∞·ªùi tham gia... ({num_players}/{total_slots})"

    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt l·ªùi ch√†o t·ª± nhi√™n, huy·ªÅn b√≠ kho·∫£ng 2-3 c√¢u ƒë·ªÉ ƒë√≥n c√°c ng∆∞·ªùi ch∆°i t·ªõi ph√≤ng ch·ªù. Tone: b√≠ ·∫©n, ƒë√°ng s·ª£. Sau ƒë√≥ th√™m d√≤ng y√™u c·∫ßu: "ƒêang ch·ªù {num_players}/{total_slots} ng∆∞·ªùi ch∆°i x√°c nh·∫≠n..."<|im_end|>
<|im_start|>user
Vi·∫øt l·ªùi ch√†o cho ph√≤ng ch·ªù.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>"],
            echo=False,
            temperature=0.7
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)


async def generate_simple_greeting(scenario_type: str) -> str:
    """Generate a simple greeting when creating game room (preset, no LLM)."""
    greetings = {
        "asylum": "üè• M·ªôt b·ªánh vi·ªán t√¢m th·∫ßn l·∫°nh l·∫Ωo, nh·ªØng chi·∫øc gi∆∞·ªùng tr·ªëng... B·∫°n nghe ti·∫øng ƒë·ªông vang vang...",
        "factory": "üè≠ M·ªôt nh√† m√°y c≈© k·ªπ, m√°y m√≥c g·ªâ s√©t. √Ånh s√°ng m·ªù t·ª´ c·ª≠a s·ªï v·ª°...",
        "ghost_village": "üëª M·ªôt ng√¥i l√†ng hoang v·∫Øng, nh√† c·ª≠a ƒë·ªï n√°t. Gi√≥ l·∫°nh th·ªïi qua...",
        "cursed_mansion": "üè∞ M·ªôt l√¢u ƒë√†i b·ªã nguy·ªÅn r·ªßa. B√≥ng t·ªëi bao ph·ªß m·ªçi n∆°i...",
        "mine": "‚õèÔ∏è M·ªôt m·ªè than s√¢u th·∫≥m, ƒë·∫ßy v·∫øt n·ª©t. Ti·∫øng n∆∞·ªõc ch·∫£y t·ª´ d∆∞·ªõi...",
        "prison": "‚õìÔ∏è M·ªôt nh√† t√π c≈©, nh·ªØng cell s·∫Øt g·ªâ s√©t. √Çm thanh ti·∫øng la h∆°...",
        "abyss": "üåë M·ªôt v·ª±c th·∫≥m s√¢u, b√≥ng t·ªëi kh√¥ng d·ª©t. B·∫°n kh√¥ng bi·∫øt d∆∞·ªõi c√≥ g√¨...",
        "dead_forest": "üå≤ M·ªôt khu r·ª´ng ch·∫øt, c√¢y c·ªï th·ª• h√©o √∫a. V·∫´n c√≤n m√πi x√°c th·ªëi...",
        "research_hospital": "üî¨ M·ªôt b·ªánh vi·ªán nghi√™n c·ª©u b√≠ m·∫≠t, t·∫•m k√≠nh ƒëen k√≠n...",
        "ghost_ship": "‚õµ M·ªôt chi·∫øc t√†u b·ªè hoang, s√†n g·ªó m·ª•c n√°t. Ti·∫øng bi·ªÉn vang xa...",
    }
    
    return greetings.get(scenario_type, f"üìç Ph√≤ng {scenario_type} ƒë·ª£i b·∫°n kh√°m ph√°...")


async def generate_world_lore(scenario_type: str) -> str:
    """Generate detailed world lore for the scenario (can be long, will be chunked)."""
    if _llm is None:
        return "Th·∫ø gi·ªõi b√≠ ·∫©n... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† m·ªôt t√°c gi·∫£ ti·ªÉu thuy·∫øt kinh d·ªã. H√£y vi·∫øt lore chi ti·∫øt (300-400 t·ª´) cho m·ªôt th·∫ø gi·ªõi kinh d√≠ scenario '{scenario_type}'. 
Tone: huy·ªÅn b√≠, ƒë√°ng s·ª£, chi ti·∫øt, nh∆∞ c√°c ti·ªÉu thuy·∫øt Trung Qu·ªëc c·ªï.
M√¥ t·∫£: nguy√™n nh√¢n b√≠ ·∫©n, c√°c quy t·∫Øc qu·ª∑ d·ªã, nh·ªØng g√¨ ƒëang x·∫£y ra, c·∫£m gi√°c kh√≥ ch·ªãu, c√°c y·∫øu t·ªë si√™u nhi√™n.
Vi·∫øt b·∫±ng ti·∫øng Vi·ªát, gi·ªØ √¢m h∆∞·ªüng ma qu√°i.<|im_end|>
<|im_start|>user
Vi·∫øt lore chi ti·∫øt cho scenario n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=500,
            stop=["<|im_end|>"],
            echo=False,
            temperature=0.8
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)
