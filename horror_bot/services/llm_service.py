import asyncio
import os
from dotenv import load_dotenv
try:
    from llama_cpp import Llama
except ImportError:
    print("Lá»—i: ChÆ°a cÃ i llama-cpp-python. HÃ£y cháº¡y pip install -r requirements.txt")
    Llama = None

load_dotenv()

LLM_MODEL_PATH = os.getenv("LLM_MODEL_PATH")
n_threads = int(os.getenv("LLM_N_THREADS", "2"))
n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "4096"))

# Global model instance
_llm = None

def load_llm():
    global _llm
    if _llm is not None:
        return True

    if not LLM_MODEL_PATH or not os.path.exists(LLM_MODEL_PATH):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y model táº¡i: {LLM_MODEL_PATH}")
        print("ğŸ‘‰ HÃ£y cháº¡y python download_model.py trÆ°á»›c.")
        return False

    try:
        print(f"ğŸ”„ Äang load model GGUF (Threads: {n_threads}, Context: {n_ctx})...")
        _llm = Llama(
            model_path=LLM_MODEL_PATH,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_gpu_layers=0, # Cháº¡y thuáº§n CPU
            verbose=False
        )
        print("âœ… LLM Load thÃ nh cÃ´ng!")
        return True
    except Exception as e:
        print(f"âŒ Lá»—i load model: {e}")
        return False

async def describe_scene(keywords: list[str]) -> str:
    if _llm is None:
        return "KhÃ´ng gian tÄ©nh má»‹ch... (AI chÆ°a load)"

    # Prompt tá»‘i Æ°u cho Qwen Instruct
    prompt = f"""<|im_start|>system
Báº¡n lÃ  quáº£n trÃ² game kinh dá»‹. HÃ£y viáº¿t má»™t Ä‘oáº¡n vÄƒn mÃ´ táº£ ngáº¯n (dÆ°á»›i 50 tá»«) dá»±a trÃªn cÃ¡c tá»« khÃ³a: {', '.join(keywords)}. Giá»ng vÄƒn u Ã¡m, Ä‘Ã¡ng sá»£.<|im_end|>
<|im_start|>user
MÃ´ táº£ cáº£nh nÃ y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    # Cháº¡y trong thread pool Ä‘á»ƒ khÃ´ng cháº·n bot Discord
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>", "\n\n"],
            echo=False,
            temperature=0.7
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)