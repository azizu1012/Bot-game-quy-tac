import asyncio
import os
from dotenv import load_dotenv
try:
    from llama_cpp import Llama
except ImportError:
    print("L·ªói: Ch∆∞a c√†i llama-cpp-python. H√£y ch·∫°y pip install -r requirements.txt")
    Llama = None

load_dotenv()

LLM_MODEL_PATH = os.getenv("LLM_MODEL_PATH")
n_threads = int(os.getenv("LLM_N_THREADS", "2"))
n_ctx = int(os.getenv("LLM_CONTEXT_SIZE", "4096"))

# Global model instance
_llm = None

def load_llm():
    global _llm
    if _llm is not None:
        return True

    if not LLM_MODEL_PATH or not os.path.exists(LLM_MODEL_PATH):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y model t·∫°i: {LLM_MODEL_PATH}")
        print("üëâ H√£y ch·∫°y python download_model.py tr∆∞·ªõc.")
        return False

    try:
        print(f"üîÑ ƒêang load model GGUF (Threads: {n_threads}, Context: {n_ctx})...")
        _llm = Llama(
            model_path=LLM_MODEL_PATH,
            n_ctx=n_ctx,
            n_threads=n_threads,
            n_gpu_layers=0, # Ch·∫°y thu·∫ßn CPU
            verbose=False
        )
        print("‚úÖ LLM Load th√†nh c√¥ng!")
        return True
    except Exception as e:
        print(f"‚ùå L·ªói load model: {e}")
        return False

async def describe_scene(keywords: list[str]) -> str:
    if _llm is None:
        return "Kh√¥ng gian tƒ©nh m·ªãch... (AI ch∆∞a load)"

    # Prompt t·ªëi ∆∞u cho Qwen Instruct
    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ ng·∫Øn (d∆∞·ªõi 50 t·ª´) d·ª±a tr√™n c√°c t·ª´ kh√≥a: {', '.join(keywords)}. Gi·ªçng vƒÉn u √°m, ƒë√°ng s·ª£.<|im_end|>
<|im_start|>user
M√¥ t·∫£ c·∫£nh n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    # Ch·∫°y trong thread pool ƒë·ªÉ kh√¥ng ch·∫∑n bot Discord
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>", "\n\n"],
            echo=False,
            temperature=0.7
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)

async def describe_scene_stream(keywords: list[str], callback=None) -> str:
    """Generate scene description v·ªõi streaming callback (g·ªçi callback t·ª´ng ph·∫ßn)."""
    if _llm is None:
        return "Kh√¥ng gian tƒ©nh m·ªãch... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt ƒëo·∫°n vƒÉn m√¥ t·∫£ ng·∫Øn (d∆∞·ªõi 50 t·ª´) d·ª±a tr√™n c√°c t·ª´ kh√≥a: {', '.join(keywords)}. Gi·ªçng vƒÉn u √°m, ƒë√°ng s·ª£.<|im_end|>
<|im_start|>user
M√¥ t·∫£ c·∫£nh n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>", "\n\n"],
            echo=False,
            temperature=0.7
        )
        result = output['choices'][0]['text'].strip()
        
        # N·∫øu c√≥ callback, g·ªçi callback v·ªõi t·ª´ng c√¢u
        if callback:
            sentences = result.split('. ')
            for i, sentence in enumerate(sentences):
                callback(sentence + ('.' if i < len(sentences) - 1 else ''))
        
        return result

    return await loop.run_in_executor(None, run_inference)

async def generate_dark_rules(scenario_type: str) -> str:
    """Generate a set of dark rules for the game scenario like Chinese novels."""
    if _llm is None:
        return "Kh√¥ng c√≥ quy t·∫Øc... (AI ch∆∞a load)"

    prompt = f"""<|im_start|>system
B·∫°n l√† t√°c gi·∫£ ti·ªÉu thuy·∫øt kinh d·ªã ch√¢u √Å. H√£y t·∫°o 3-4 quy t·∫Øc ma qu√°i, u √°m cho m·ªôt tr√≤ ch∆°i kinh d√≠ trong scenario '{scenario_type}'. Vi·∫øt d∆∞·ªõi d·∫°ng danh s√°ch v·ªõi tone mu·ªën r·ª£n ng∆∞·ªùi, huy·ªÅn b√≠, gi·ªëng nh∆∞ c√°c ti·ªÉu thuy·∫øt Trung Qu·ªëc c·ªï. Gi·ªØ ng·∫Øn g·ªçn, m·ªói quy t·∫Øc 1-2 c√¢u.<|im_end|>
<|im_start|>user
T·∫°o nh·ªØng quy t·∫Øc qu·ª∑ d·ªã cho scenario n√†y.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=200,
            stop=["<|im_end|>", "\n\n\n"],
            echo=False,
            temperature=0.8
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)

async def generate_waiting_room_message(num_players: int, total_slots: int = 8) -> str:
    """Generate a natural greeting for waiting room."""
    if _llm is None:
        return f"ƒêang ch·ªù ƒë·ªß ng∆∞·ªùi tham gia... ({num_players}/{total_slots})"

    prompt = f"""<|im_start|>system
B·∫°n l√† qu·∫£n tr√≤ game kinh d√≠. H√£y vi·∫øt m·ªôt l·ªùi ch√†o t·ª± nhi√™n, huy·ªÅn b√≠ kho·∫£ng 2-3 c√¢u ƒë·ªÉ ƒë√≥n c√°c ng∆∞·ªùi ch∆°i t·ªõi ph√≤ng ch·ªù. Tone: b√≠ ·∫©n, ƒë√°ng s·ª£. Sau ƒë√≥ th√™m d√≤ng y√™u c·∫ßu: "ƒêang ch·ªù {num_players}/{total_slots} ng∆∞·ªùi ch∆°i x√°c nh·∫≠n..."<|im_end|>
<|im_start|>user
Vi·∫øt l·ªùi ch√†o cho ph√≤ng ch·ªù.<|im_end|>
<|im_start|>assistant
"""

    loop = asyncio.get_running_loop()
    
    def run_inference():
        output = _llm(
            prompt,
            max_tokens=150,
            stop=["<|im_end|>"],
            echo=False,
            temperature=0.7
        )
        return output['choices'][0]['text'].strip()

    return await loop.run_in_executor(None, run_inference)